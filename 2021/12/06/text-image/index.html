<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>text-image | Hexo</title>
  <meta name="description" content="1、Learning Transferable Visual Models From Natural Language Supervision（CLIP）文章链接：2103.00020.pdf Main idea:-现有的state-of-the-art cv方法一般用于预测预先定义好的object 类别，由于在用于其他视觉任务时需要引入额外的标注，导致这些模型在泛化性较差。 -通过描述image">
<meta property="og:type" content="article">
<meta property="og:title" content="text-image">
<meta property="og:url" content="http://example.com/2021/12/06/text-image/index.html">
<meta property="og:site_name" content="Carlos Homepage">
<meta property="og:description" content="1、Learning Transferable Visual Models From Natural Language Supervision（CLIP）文章链接：2103.00020.pdf Main idea:-现有的state-of-the-art cv方法一般用于预测预先定义好的object 类别，由于在用于其他视觉任务时需要引入额外的标注，导致这些模型在泛化性较差。 -通过描述image">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211207111207094.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211208103142349.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211208105431946.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211208152736592.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211208161252055.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211208161313607.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211208174614067.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211208173644174.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211208174752304.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211221151755897.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211221153119816.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211221155052497.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211221171019691.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211221172726779.png">
<meta property="og:image" content="http://example.com/2021/12/06/text-image/image-20211222121717390.png">
<meta property="article:published_time" content="2021-12-06T09:14:49.000Z">
<meta property="article:modified_time" content="2021-12-22T10:36:43.179Z">
<meta property="article:author" content="Carlos">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/12/06/text-image/image-20211207111207094.png">
  <!-- Canonical links -->
  <link rel="canonical" href="http://example.com/2021/12/06/text-image/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Carlos Homepage" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 5.4.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/cynblog" target="_blank">
          <img class="img-circle img-rotate" src="/images/touxiang.jpeg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Carlos</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">Computer Life</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Hefei, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/blog" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>欢迎交流与分享经验!</p>
            </div>
        </div>
    </div>
</div>

    
      

    
      
  <div class="widget">
    <h3 class="widget-title">标签</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/self-supervised/" rel="tag">self-supervised</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/self-supervised/" style="font-size: 13px;">self-supervised</a> <a href="/tags/tools/" style="font-size: 13px;">tools</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2021/12/15/tools/" class="title">tools</a>
              </p>
              <p class="item-date">
                <time datetime="2021-12-15T06:55:59.000Z" itemprop="datePublished">2021-12-15</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2021/12/06/text-image/" class="title">text-image</a>
              </p>
              <p class="item-date">
                <time datetime="2021-12-06T09:14:49.000Z" itemprop="datePublished">2021-12-06</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2021/12/06/self-supervised/" class="title">self-supervised</a>
              </p>
              <p class="item-date">
                <time datetime="2021-12-06T09:09:39.000Z" itemprop="datePublished">2021-12-06</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2021/11/26/hello-world/" class="title">Hello World</a>
              </p>
              <p class="item-date">
                <time datetime="2021-11-26T08:45:04.473Z" itemprop="datePublished">2021-11-26</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-text-image" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      text-image
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2021/12/06/text-image/" class="article-date">
	  <time datetime="2021-12-06T09:14:49.000Z" itemprop="datePublished">2021-12-06</time>
	</a>
</span>
        
        

        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2021/12/06/text-image/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h1 id="1、Learning-Transferable-Visual-Models-From-Natural-Language-Supervision（CLIP）"><a href="#1、Learning-Transferable-Visual-Models-From-Natural-Language-Supervision（CLIP）" class="headerlink" title="1、Learning Transferable Visual Models From Natural Language Supervision（CLIP）"></a>1、Learning Transferable Visual Models From Natural Language Supervision（CLIP）</h1><p>文章链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.00020.pdf">2103.00020.pdf</a></p>
<h3 id="Main-idea"><a href="#Main-idea" class="headerlink" title="Main idea:"></a>Main idea:</h3><p>-现有的state-of-the-art cv方法一般用于预测预先定义好的object 类别，由于在用于其他视觉任务时需要引入额外的标注，导致这些模型在泛化性较差。</p>
<p>-通过描述image的text可以提供更加广泛的监督。这篇文章提出了一个简单的预训练任务，预测image的caption，学习到了有效的可泛化的特征。利用了400million（image， text）对。经过预训练可以实现zero-shot transfer to downstream tasks。</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction:"></a>Introduction:</h3><p> Scalable pre-training的NLP方法通过从大量的网络text中学习取得了有效的有突破性的进步。是否能够在cv领域取得类似的成就？</p>
<p>-之前有一些工作利用text辅助学习image representations。</p>
<p>[1] Learning visual representations from textual annotations</p>
<p>[2] Learning visual representations with caption annotations.</p>
<p>[3] Contrastive learning of medical visual representations from paired images and text. </p>
<p>-有关自然语言辅助的图片特征学习的证明比较少，可能是因为之前的方法仅仅取得了有限的有效性。</p>
<p>-之前的方法使用的是固定的softmax分类器，没法动态的输出，导致了zero-shot的预测能力有限。</p>
<p>-之前的text监督的image representation 学习的数据量有限</p>
<p>这篇文章通过大量的互联网上可用的数据创建了一个400million的（image，text）数据对，并提出contrastive language-image pre-training方法，学习到了有效的特征。</p>
<h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach:"></a>Approach:</h3><h4 id="Natural-language-superviseion"><a href="#Natural-language-superviseion" class="headerlink" title="Natural language superviseion"></a>Natural language superviseion</h4><p>优点：自然语言不需要像传统机器学习方法那样进行标注，更加容易获取监督数据。通过自然语言学习特征，还可以进行zero-shot transfer。</p>
<h4 id="Creating-a-sufficiently-large-dataset"><a href="#Creating-a-sufficiently-large-dataset" class="headerlink" title="Creating a sufficiently large dataset"></a>Creating a sufficiently large dataset</h4><p>之前的方法没有充分的利用网络上的image数据，这个工作充分的利用了网络上的数据构建了一个足够大的image-text数据集。</p>
<h4 id="Selecting-an-Efficient-Pre-Training-Method"><a href="#Selecting-an-Efficient-Pre-Training-Method" class="headerlink" title="Selecting an Efficient Pre-Training Method"></a>Selecting an Efficient Pre-Training Method</h4><p>之前的方法往往直接预测与图片相关的文本中的确定的words，由于文本变化的多样性，这是一个非常难的任务。这个工作通过对比的方式，匹配text和image，不是通过预测具体的词，实现了有效的提升。</p>
<p><img src="/2021/12/06/text-image/image-20211207111207094.png" alt="image-20211207111207094"></p>
<p>具体来说，给定一个batch，包括N个image，text对，CLIP预测这个batch中N*N个可能的对中真实的对。CLIP通过同时训练一个image encoder和text encoder学习了一个多模态的embedding space。最大化N个真实数据对的cosine similarity，最小化N * N - N不正确数据对的相似性。</p>
<p>image encoder使用了两种，基于resnet，以及基于ViT。</p>
<p>text encoder使用的是transformer。</p>
<h1 id="2、Learning-Visual-Representations-with-Caption-Annotations"><a href="#2、Learning-Visual-Representations-with-Caption-Annotations" class="headerlink" title="2、Learning Visual Representations with Caption Annotations"></a>2、Learning Visual Representations with Caption Annotations</h1><p>文章链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.01392">arXiv:2008.01392</a></p>
<h3 id="Main-idea-1"><a href="#Main-idea-1" class="headerlink" title="Main idea:"></a>Main idea:</h3><p>近期的预训练相关的工作开始利用带有噪声，甚至是无标注的数据进行预训练。带有标题的图片是容易获取的，这种信息可以用来监督视觉特征的学习。这个工作提出利用image-caption对学习视觉特征（image-conditioned masked language modeling(ICMLM)）。ICMLIM 基于视觉线索预测caption中masked words。</p>
<h3 id="Introduction："><a href="#Introduction：" class="headerlink" title="Introduction："></a>Introduction：</h3><p>基于监督学习的方法加快了CV的发展，但是存在两个问题，一个是标注的数据不易获取，另外一个是对新任务的迁移比较麻烦。自监督方法同样需要大量的计算资源才能实现较好的实验结果。</p>
<p>幸运的是captioned image数据是比较多的。这个工作认为通过captions学习视觉特征可以大幅度减少对训练数据集的需求。而且即使没有文本，也可以容易地自动构建文本信息。之前的方法没有很好的考虑这一文本信息。为了利用文本信息学习视觉特征，这个工作提出了两个proxy tasks，一个是预测image tags，一个是通过masked captions学习视觉特征。通过可视化可以发现第二个任务能够有效的利用文本，并且较好的关注到图像的特定regions。</p>
<img src="/2021/12/06/text-image/image-20211208103142349.png" alt="image-20211208103142349" style="zoom:50%;">



<h3 id="Approach："><a href="#Approach：" class="headerlink" title="Approach："></a>Approach：</h3><img src="/2021/12/06/text-image/image-20211208105431946.png" alt="image-20211208105431946" style="zoom:50%;">

<p>这个工作假定文本信息能够辅助提升视觉特征的学习，构建了两个proxy任务。这两个任务能够利用文本的结构信息。</p>
<p>包括两个proxy tasks。第一个task预测image-level tags，学习global层的image 信息。第二个task通过mask word，学习local层的image语义信息。</p>
<p>框图包括五块，（1）提出视觉特征。（2）编码语言的token特征。（3）tfm, 利用 transformer从visual and text feature预测masked token （4）只基于visual feature预测masked token。（5）tag prediction </p>
<h4 id="Capturing-image-level-semantics"><a href="#Capturing-image-level-semantics" class="headerlink" title="Capturing image-level semantics"></a>Capturing image-level semantics</h4><p>模块1+5。根据图像中可能出现的concepts，构建一个multi-label图像分类任务。基于文本，构建一个multi-label 的label vector，$ \left { 0, 1 \right }^K $。这些label可以被看作是tags，因此预训练任务为tag prediction。这个工作考虑了多个构建tags的方法，tag表示了文本的global信息。（语言模型不熟，后面再学习）</p>
<h4 id="Capturing-localized-semantics"><a href="#Capturing-localized-semantics" class="headerlink" title="Capturing localized semantics"></a>Capturing localized semantics</h4><p>扩展masked language model到image-conditioned 情况。通过image 特征预测masked tokens。以使得学习到的视觉特征能够学习足够的信息，重构出caption中的missing information。两种方式：</p>
<p>(1)+(2)+(3) 基于transformer，利用visual 和text feature预测masked token。</p>
<p>(1)+(2)+(4) 前一种方式由于有text信息，text信息已经足够预测masked信息，可能会导致视觉特征学习的比较差，这里只利用视觉特征预测token。（4）中包括两步，localizing concept和predicting labels。localizing输入视觉特征和文本特征，计算attention map。利用attention map和视觉特征预测masked token。</p>
<h1 id="3、CONTRASTIVE-LEARNING-OF-MEDICAL-VISUAL-REPRESENTATIONS-FROM-PAIRED-IMAGES-AND-TEXT"><a href="#3、CONTRASTIVE-LEARNING-OF-MEDICAL-VISUAL-REPRESENTATIONS-FROM-PAIRED-IMAGES-AND-TEXT" class="headerlink" title="3、CONTRASTIVE LEARNING OF MEDICAL VISUAL REPRESENTATIONS FROM PAIRED IMAGES AND TEXT"></a>3、CONTRASTIVE LEARNING OF MEDICAL VISUAL REPRESENTATIONS FROM PAIRED IMAGES AND TEXT</h1><p>文章链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.00747">arXiv:2010.00747</a></p>
<h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>现有的medical image understanding主要受限于标注数据不足的问题。为了解决数据的问题，现有的方法包括两种。第一种利用ImageNet数据集上预训练的参数初始化，但是imagenet上的数据和medical image有着比较大的差异，并没有起到很好的效果。第二种利用容易获取的带有medical image的文本报告总结expert-crafted rules。但是这种基于规则的标注方法有两个限制，一方面规则往往是不准确的，另一方面这些规则往往是领域特定的，不利于cross-domain。之前也有方法提出基于图像的对比学习方法，但是仅仅取得了有限的效果。</p>
<p>这篇文章旨在于联合文本数据和无监督方法，改进medical images的特征学习。方法意图最大化真实image和text之间的一致性。实现了比较好的特征学习结果。</p>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><img src="/2021/12/06/text-image/image-20211208152736592.png" alt="image-20211208152736592" style="zoom:50%;">



<p>输入为 $ X_v，X_u $对，$X_v$表示images，$X_u$ 表示对应的描述文本。目标是学习一个image encoder $f_v$ 编码图像的特征。在实际的场景中，成对的text-image对比较容易获取。对输入$ X_v，X_u $经过image 和text encoder得到提取的特征$h_v, h_u$。类似contrastive learning经过projection function得到high-level feature v 和u。对比学习包括两个loss：</p>
<p>对于每个$v_i$，计算对比学习损失，使得对应文本$u_i$的相似性接近，$v_i$非配对的$u_k$相似性变远。</p>
<img src="/2021/12/06/text-image/image-20211208161252055.png" alt="image-20211208161252055" style="zoom:50%;">

<p>对于每个$u_i$，计算对比学习损失，使得对应图像$v_i$的相似性接近，$u_i$非配对的$v_k$相似性变远。</p>
<img src="/2021/12/06/text-image/image-20211208161313607.png" alt="image-20211208161313607" style="zoom:50%;">



<h1 id="4、VirTex-Learning-Visual-Representations-from-Textual-Annotations"><a href="#4、VirTex-Learning-Visual-Representations-from-Textual-Annotations" class="headerlink" title="4、VirTex: Learning Visual Representations from Textual Annotations"></a>4、VirTex: Learning Visual Representations from Textual Annotations</h1><p>论文链接：<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Desai_VirTex_Learning_Visual_Representations_From_Textual_Annotations_CVPR_2021_paper.pdf">VirTex</a></p>
<h4 id="Main-idea-2"><a href="#Main-idea-2" class="headerlink" title="Main idea:"></a>Main idea:</h4><p>这篇文章提出利用语义密集的文本信息学习视觉特征。学习到了比较好的结果。</p>
<h4 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h4><p>之前的方法大多是利用在ImageNet等数据集上预训练的网络参数。但这种方法需要人工标注的images。最近的方法开始利用无标注的images辅助学习视觉特征，这种方法需要海量的数据。这个工作企图利用更少的image学习高质量的特征。</p>
<p><img src="/2021/12/06/text-image/image-20211208174614067.png" alt="image-20211208174614067"></p>
<p>如Fig 2，相比unsupervised contrastive learning以及有监督分类，文本能够提供高语义密度的信息。并且文本是更加容易收集的。</p>
<img src="/2021/12/06/text-image/image-20211208173644174.png" alt="image-20211208173644174" style="zoom:50%;">

<p>这个工作提出利用caption 标注学习视觉特征，如Fig 1。这里利用一个convnet和一个transformer生成图片的captions。之后将这个学到的特征transfer到下游的视觉识别任务中。</p>
<h4 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h4><p><img src="/2021/12/06/text-image/image-20211208174752304.png" alt="image-20211208174752304"></p>
<p>方法试图通过训练image captioning 模型预测图片的caption信息，如Fig 3。模型包括两个部分，visual backbone用于提取图片的视觉特征，textual head用于利用提取的视觉特征token by token的预测caption。textual head 包括前向和后向预测transformer网络分别left-to-right和right-to-left预测caption。</p>
<p>下游任务中仅仅使用visual backbone网络。</p>
<h1 id="5、Oscar-Object-Semantics-Aligned-Pre-training-for-Vision-Language-Tasks"><a href="#5、Oscar-Object-Semantics-Aligned-Pre-training-for-Vision-Language-Tasks" class="headerlink" title="5、Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"></a>5、Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</h1><p>论文<a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750120.pdf">链接</a></p>
<h4 id="Main-idea-3"><a href="#Main-idea-3" class="headerlink" title="Main idea"></a>Main idea</h4><p>之前的方法采用强行对准的方式，使得image-text semantic alignment，这个工作利用图片中的objects tags 作为anchor points，减轻对齐的难度。这一方法受启发于图片中的突出object往往会被准确的检测，并在对应的文本描述中高频率的被提及。</p>
<h4 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h4><p>学习跨模态的特征在视觉语言任务中非常重要。视觉语言预训练方法已经证明了其有效性。（VLP）</p>
<p>之前的VLP方法诉诸于利用self-attention 机制学习semantic alignment between image regions and text。然而image region和text pose之间缺少明确的alignment information。这使得图像区域和文本之间的alignment变的比较困难。</p>
<p>这个文章表明，引入图片中的object tags能够减轻image-text semantic alignment的难度。这个工作提出了一种新的VLP方法，训练samples是一个三元组，（word sequence， object tags， image region feature）。</p>
<h4 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h4><img src="/2021/12/06/text-image/image-20211221151755897.png" alt="image-20211221151755897" style="zoom:50%;">

<p>如图Fig.2a，图像文本任务的输入一般为image-text 对。现有的基于image-text的预训练方法主要有两个问题：</p>
<p>1、visual region feature可能存在重叠，因此提出的视觉特征具有模糊性。</p>
<p>2、由于缺少图片和text的明确对应，region feature和text之间的对应是不明确的，这种对应通过网络自学，是一个弱监督的问题。</p>
<p>因此这篇文章提出利用图片中的salient object的tag作为anchor points，使得image region和textual units semantic alignment。</p>
<h4 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h4><p>动机：text-image中的重要信息会被同时包含。因此目标是同时捕捉两个模态间共有的信息。</p>
<img src="/2021/12/06/text-image/image-20211221153119816.png" alt="image-20211221153119816" style="zoom:50%;">

<p>方法的输入是三元组（w, q, v），分别对应于text embedding sequence, object tags(text)，region vectors of images。通过引入object tags，使得object 区域的权重更大，减轻text，region feature对准的难度。这里利用Faster R-CNN提取图片中多个区域的特征，并加上region position作为region feature。进一步利用Faster R-CNN检测高准确度的object tags。</p>
<p>预训练：</p>
<p>三元组可以从两个不同的方面来看待：</p>
<img src="/2021/12/06/text-image/image-20211221155052497.png" alt="image-20211221155052497" style="zoom:50%;">

<p>a dictionary view: 此时文本tokens和object tags共享相同的语义空间，image region feature位于viusal semantic space。利用masked token loss进行预训练。在（w, q）中随机mask token，基于unmasked tokens和image feature v预测masked tokens。</p>
<p>a modality view：此时（q, v）表示image modality， w为language modality。这里sample出一组污染的image representations，以50%随机取代image representation中的q。基于transformer 【cls】位的输出预测特征是orginal或者polluted。这里可以理解为基于w, v预测q是否一致。</p>
<h1 id="6、VIVO-Visual-Vocabulary-Pre-Training-for-Novel-Object-Captioning"><a href="#6、VIVO-Visual-Vocabulary-Pre-Training-for-Novel-Object-Captioning" class="headerlink" title="6、VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning"></a>6、VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning</h1><p>论文<a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/16249/16056">链接</a></p>
<h4 id="Main-idea："><a href="#Main-idea：" class="headerlink" title="Main idea："></a>Main idea：</h4><p>当描述一个training data中没有的novel objects时，现有的VLP方法就不可用了。这个工作提出了一种方法在缺少caption 标注的情况下也能进行预训练。方法能够利用大量的成对的image-tag数据学习一个visual vocabulary。方法预训练了一个多层multilayer transformer模型，利用相应的image feature学习预测image-level的tags。</p>
<h4 id="Introduction-4"><a href="#Introduction-4" class="headerlink" title="Introduction"></a>Introduction</h4><p>之前的image captioning方法在in-the-wild数据上没有很好的结果。nocaps benchmark被构建，用于评估unseen data对captioning 方法的影响。这个工作提出利用大量可用的没有caption的vision 数据学习visual vocabulary。在预训练时，语义信息相近的tags，image region feature被映射到相近的vectors。然后利用image-caption对微调model，在inference时可以推测出fine-tuning中没有见到的object。</p>
<img src="/2021/12/06/text-image/image-20211221171019691.png" alt="image-20211221171019691" style="zoom:50%;">

<h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><p><img src="/2021/12/06/text-image/image-20211221172726779.png" alt="image-20211221172726779"></p>
<p>方法利用大量的image-tag 对预训练image captioning models。方法包括两个stages。第一阶段image captions model 学习利用tags标注image regions。第二阶段给定image-captions对和detected object tags，模型学习基于detected objects将image map为一个sentence。由于tags相比较于caption有着更多的类别，因此能够对没见过的object caption到正确的信息。模型基于multiple transformer layer。</p>
<p>Pretraining：</p>
<p>在一个large-scale dataset with abundant tags。这里的tags指的是图片中的visual objects。pretraining中将提取的图片中的多个特征和tag tokens输入到transformer中，训练时随机的mask一些token，让model去预测masked token。这里为了考虑tag顺序，利用了一个the Hungarian matching loss。</p>
<p>Fine-tuning和inference</p>
<p>fine-tuning给定image regions，tags，models学习预测某些位置被mask的caption token。目标通过image，tags，caption预测caption中masked tokens。</p>
<p>inference时首先提取图片的region feature，然后检测image的tags。然后逐个的输出token。</p>
<h1 id="7、ClipCap-CLIP-Prefix-for-Image-Captioning"><a href="#7、ClipCap-CLIP-Prefix-for-Image-Captioning" class="headerlink" title="7、ClipCap: CLIP Prefix for Image Captioning"></a>7、ClipCap: CLIP Prefix for Image Captioning</h1><h4 id="Main-idea-4"><a href="#Main-idea-4" class="headerlink" title="Main idea:"></a>Main idea:</h4><p>利用clip提取特征，基于GPT2生成caption。</p>
<h4 id="Introduction-5"><a href="#Introduction-5" class="headerlink" title="Introduction"></a>Introduction</h4><p>现有的image caption方法需要学习visual 和textual特征之间的关联，所以需要大量的数据和时间才能进行训练。这在实际的应用中也有诸多限制。因此希望有一个lightweight模型，能够有更少的训练参数，更快的训练时间。这个工作提出利用训练好的clip的encoder训练image captioning model。可以在非常短的时间内训练好网络。</p>
<h4 id="Method-3"><a href="#Method-3" class="headerlink" title="Method"></a>Method</h4><p><img src="/2021/12/06/text-image/image-20211222121717390.png" alt="image-20211222121717390"></p>
<p>首先利用CLIP提取图片的特征，然后利用一个mapping network将clip的特征映射为多个prefix embeddings。将predix embedding和caption tokens一起输入到GPT2中预测captions，计算损失。</p>
<p>这里的mapping network是基于transformer结构，输入除了clip embedding，还有一个learned constant。mapping network将clip embbedings 映射到gpt2 space。训练时只优化mapping network就可以实现不错的caption 结果。</p>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://example.com/2021/12/06/text-image/" title="text-image" target="_blank" rel="external">http://example.com/2021/12/06/text-image/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/cynblog" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/touxiang.jpeg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/cynblog" target="_blank"><span class="text-dark">Carlos</span><small class="ml-1x">Computer Life</small></a></h3>
        <div>一个搬过砖的程序猿</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2021/12/15/tools/" title="tools"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2021/12/06/self-supervised/" title="self-supervised"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/blog" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: '',
    appKey: '',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script>

     







</body>
</html>